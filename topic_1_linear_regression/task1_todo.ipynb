{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание №1\n",
    "\n",
    "## Теория\n",
    "\n",
    "Этот ноутбук содержит вспомогательный материал для выполнения первого задания, в котором надо предсказать целевую переменную по двум признакам с помощью линейной регрессии. Хочу сразу предупредить, что здесь собрана вводная информация, причем в некоторых аспектах я намеренно привел упрощенное или не до конца полное описание. В дальнейшем мы отдельно размерем все эти аспекты.\n",
    "\n",
    "Для начала, вспомним, что такое линейная регрессия и как выглядит алгоритм подбора оптимальных параметров предсказания.\n",
    "\n",
    "Назовем множество имеющихся примеров **обучающей выборкой** или **тренировочными данными**. Обучающая выборка — это множество пар $D = \\{(\\vec{x^{(i)}},y^{(i)})\\}_{i=1,2,...,n}$, где $n$ - размер выборки. В каждой паре через $y \\in R$ обозначена целевая переменная (или ответ), который соответствует вектору признаков $\\vec{x} \\in R^{m\\times1}$. Необходимо научиться предсказывать значение $y$ по $\\vec{x}$. Обозначим предсказанное значение через $\\hat{y}$. В линейной регрессии предсказание делается с помощью следующей формулы:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = h(\\vec{x},\\vec{\\theta}) = \\vec{x}^T\\vec{\\theta} = \\langle \\vec{x}, \\vec{\\theta} \\rangle = \\sum_{j=1}^{m} x_j\\theta_j,\n",
    "\\end{equation}\n",
    "\n",
    "где через $h(\\vec{x},\\vec{\\theta})$ обозначена функция предсказания, а $\\vec{\\theta} \\in R^{m\\times1}$ — вектор параметров линейной регрессии. **Обучение** модели на основе линейной регрессии заключается в подборе $\\vec{\\theta}$ так, чтобы предсказанные значения $\\hat{y}^{(i)}$ были как можно ближе к целевым переменным $y^{(i)}$ из обучающей выборки. Термин \"ближе\" значит, что есть способ измерения похожести $\\hat{y}^{(i)}$ и $y^{(i)}$. В рассматриваемой задаче в качестве критерия похожести можно воспользоваться квадратичным отклонением:\n",
    "\n",
    "\\begin{equation}\n",
    "e^{(i)} = \\left( y^{(i)} - \\hat{y}^{(i)} \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Значение $e^{(i)}$, или ошибка предсказания, показывает насколько сильно предсказанное значение целевой переменной отличается от требуемого значения для $i$-го примера обучающей выборки. Средняя ошибка предсказания, или **эмпирический риск**, на обучающей выборке определяется как \n",
    "\n",
    "\\begin{equation}\n",
    "E(D, \\vec{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} e^{(i)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y^{(i)} - h(\\vec{x^{(i)}},\\vec{\\theta}) \\right)^2 = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y^{(i)} - \\sum_{j=1}^{m} x^{(i)}_j\\theta_j \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Таким образом, задача обучения формулируется как \"найти такой вектор параметров линейной регрессии, при котором минимальна средняя ошибка предсказания\". Для поиска оптимального $\\vec{\\theta}$ можно воспользоваться методом градиентного спуска.\n",
    "\n",
    "Рассмотрим идею этого метода. \"Градиент\" функции в точке показывает направление роста функции в этой точке. Фактически, градиент -- это обобщение понятия производной на многомерный случай. Для рассмотренной нами функции эмпирического риска:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla E(D, \\vec{\\theta}) \\big\\vert_{\\vec{\\theta}=\\vec{t}} = \n",
    "\\left(\n",
    "\\begin{aligned} \n",
    "& \\frac{\\partial E(D, \\vec{\\theta})}{\\partial \\theta_0} \\big\\vert_{\\vec{\\theta}=\\vec{t}} \\\\\n",
    "& \\frac{\\partial E(D, \\vec{\\theta})}{\\partial \\theta_1} \\big\\vert_{\\vec{\\theta}=\\vec{t}} \\\\\n",
    "& ... \\\\\n",
    "& \\frac{\\partial E(D, \\vec{\\theta})}{\\partial \\theta_m} \\big\\vert_{\\vec{\\theta}=\\vec{t}} \\\\\n",
    "\\end{aligned}\n",
    "\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Нас интересует минимизация эмпирического риска. Это значит, что можно выбрать некоторое, например случайное, значение вектора $\\vec{\\theta}$, посчитать в нем значение градиента, а потом изменить $\\vec{\\theta}$ так, чтобы значение фунцкии средней ошибки уменьшилось, т.е. изменить $\\vec{\\theta}$ в направлении, обратном направлению градиента. Далее повторить эту процедуру для измененного $\\vec{\\theta}$. Если функция эмпирического риска является выпуклой и унимодальной (имеет один оптимум), то подобный итеративный пересчет через достаточное число итераций (спусков) приведет к значению параметров $\\vec{\\theta}$, в которых достигается минимум средней ошибки предсказания.\n",
    "\n",
    "Опишем теперь алгоритм поиска оптимальных параметров линейной регрессии с помощью градиентного спуска.\n",
    "\n",
    "**Инициализация:**\n",
    "- задать $\\alpha$ - скорость градиентного спуска;\n",
    "- задать начальные значения переменных: $\\vec{\\theta}^{(0)}$ = random, k = 0.\n",
    "\n",
    "**Повторять, пока не выполнятся условия остановки**\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\theta}^{(k+1)} = \\vec{\\theta}^{(k)} - \\alpha E(D, \\vec{\\theta}) \\big\\vert_{\\vec{\\theta}=\\vec{\\theta^{(k)}}} \\\\\n",
    "k = k + 1\n",
    "\\end{equation}\n",
    "\n",
    "Условия остановки:\n",
    "- достигнуто максимальное число итераций градиентного спуска;\n",
    "- значения параметров $\\theta$ почти не меняются от итерации к итерации;\n",
    "- значение градиента близко у нулю.\n",
    "\n",
    "## Практика\n",
    "\n",
    "Перейдем теперь к практической части, где нужно реализовать линейную регрессию. Далее приведен \"скелет\" программы, в которую Вам необходимо встроить недостающие фрагменты. Места для Вашег кода помечены \"TODO\".\n",
    "\n",
    "Программа состоит из следующих блоков.\n",
    "\n",
    "1. Подключение всех необходимых библиотек.\n",
    "2. Чтение и вывод тренировочных данных.\n",
    "3. Прототипы функций, которые Вам надо заполнить.\n",
    "4. Цикл обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение библиотек.\n",
    "\n",
    "В python есть много готовых библиотек для работы с данными. Наши тренировочные данные хранятся в csv файлах и для их парсинга и отображения можно воспользоваться удобной библиотекой pandas.\n",
    "\n",
    "Удобнее всего работать с данными, представленными в формате матриц и векторов. Для работы с ними в python есть библиотека numpy.\n",
    "\n",
    "Отображение графиков можно сделать с помощью matplotlib.\n",
    "\n",
    "Удобный вывод прогресса - tqdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Import all the necessary packages\n",
    "import sys\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение и вывод тренировочных данных.\n",
    "\n",
    "Для чтения данных воспользуемся методом read_csv из pandas. Эта функция возвращает специальную структуру данных DataFrame, которая содержит много методов для вывода данных и расчета различных статистик по ним. Так как данные разбиты на 2 файла, перед выводом на экран первых десяти сэмплов, сконкатенируем их в один DataFrame.\n",
    "\n",
    "Затем преобразуем данные из DataFrame в матрицы numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.98</td>\n",
       "      <td>6.459</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.52</td>\n",
       "      <td>6.193</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.74</td>\n",
       "      <td>6.750</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.81</td>\n",
       "      <td>7.249</td>\n",
       "      <td>35.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.06</td>\n",
       "      <td>5.454</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.90</td>\n",
       "      <td>6.487</td>\n",
       "      <td>24.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.94</td>\n",
       "      <td>6.998</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.36</td>\n",
       "      <td>7.163</td>\n",
       "      <td>31.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17.44</td>\n",
       "      <td>6.749</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.56</td>\n",
       "      <td>6.975</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1     0\n",
       "0  23.98  6.459  11.8\n",
       "1  21.52  6.193  11.0\n",
       "2   7.74  6.750  23.7\n",
       "3   4.81  7.249  35.4\n",
       "4  18.06  5.454  15.2\n",
       "5   5.90  6.487  24.4\n",
       "6   2.94  6.998  33.4\n",
       "7   6.36  7.163  31.6\n",
       "8  17.44  6.749  13.4\n",
       "9   4.56  6.975  34.9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. Parse and visualize data\n",
    "# parse train data: read CSV files with train features (train_x) and train targets (train_y)\n",
    "x_train = pd.read_csv(\"train_x.csv\", header=None)\n",
    "y_train = pd.read_csv(\"train_y.csv\", header=None)\n",
    "\n",
    "# show first 10 samples\n",
    "pd.concat([x_train, y_train], axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train features: (354, 2)\n",
      "Shape of train targets: (354, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert pandas dataframe to numpy arrays and matrices and diplay the dimensions of train dataset\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "print(\"Shape of train features:\", x_train.shape)\n",
    "print(\"Shape of train targets:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прототипы функций.\n",
    "\n",
    "* predict_fn - предсказание с помощью линейной регрессии;\n",
    "* loss_fn - расчет среднего значения ошибки предсказания;\n",
    "* gradient_fn - расчет градиента в точке.\n",
    "\n",
    "Вам необходимо реализовать predict_fn и gradient_fn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Prototypes.\n",
    "\n",
    "# In this demo we will use linear regression to predict targets from features.\n",
    "# In linear regression model with parameters thetas \n",
    "# the prediction y is calculated from features x using linear combination of x and thetas.\n",
    "# For example, for the case of 2 features: \n",
    "# y = theta_0 * x_o + theta_1 * x_1\n",
    "\n",
    "# Let's define some helper functions\n",
    "\n",
    "def predict_fn(x, thetas):\n",
    "    '''\n",
    "    Predict target from features x using parameters thetas and linear regression\n",
    "    \n",
    "    param x: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return y_hat: predicted scalar value for each input samples, shape Nx1\n",
    "    '''    \n",
    "    # TODO: calculate y_hat using linear regression\n",
    "    y_hat = np.zeros((x.shape[0], 1))\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate average loss value for train dataset (x_train, y_train).\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return loss: predicted scalar value for each input samples, shape Mx1\n",
    "    '''\n",
    "    y_predicted = predict_fn(x_train, thetas)    \n",
    "    loss = np.mean(np.power(y_train - y_predicted, 2))    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def gradient_fn(x_train, y_train, thetas):\n",
    "    '''\n",
    "    Calculate gradient value for linear regression.\n",
    "    \n",
    "    param x_train: input features, shape NxM, N - number of samples to predict, M - number of features\n",
    "    param y_train: input tagrets, shape Nx1\n",
    "    param thetas: vector of linear regression parameters, shape Mx1\n",
    "    return g: predicted scalar value for each input samples, shape Mx1\n",
    "    '''  \n",
    "    # TODO: calculate vector gradient\n",
    "    g = np.zeros_like(thetas)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цикл обучения\n",
    "\n",
    "Ниже дана упрощенная реализация градиентного спуска для поиска оптимальных параметров линейной регрессии. Вам необходимо подобрать значения скорости (alpha), максимальное число итераций (MAX_ITER) и условия остановки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100000/100000 [00:01<00:00, 68118.77it/s, loss_val=588.6201, thetas=-1.5970 1.5719]\n"
     ]
    }
   ],
   "source": [
    "# Step 4. Gradient descent.\n",
    "\n",
    "# now let's find optimal parameters using gradient descent\n",
    "MAX_ITER = 100000\n",
    "thetas = np.random.randn(2, 1)\n",
    "alpha = 1e-3\n",
    "\n",
    "progress = tqdm.tqdm(range(MAX_ITER), \"Training\", file=sys.stdout)\n",
    "loss_val = loss_fn(x_train, y_train, thetas)\n",
    "progress.set_postfix(loss_val=loss_val)\n",
    "\n",
    "for iter in progress:\n",
    "    gradient = gradient_fn(x_train, y_train, thetas)\n",
    "    thetas = thetas - alpha*gradient\n",
    "    \n",
    "    # TODO: add stop conditions\n",
    "    # if stop_condition is True:\n",
    "    #     progress.close()\n",
    "    #     loss_val = loss_fn(x_train, y_train, thetas)\n",
    "    #     print(\"Stop condition detected\")\n",
    "    #     print(\"Final loss:\", loss_val)\n",
    "    #     break\n",
    "    \n",
    "    if iter % 100 == 0:\n",
    "        loss_val = loss_fn(x_train, y_train, thetas)\n",
    "        progress.set_postfix(loss_val=f\"{loss_val:8.4f}\", thetas=f\"{thetas[0][0]:5.4f} {thetas[1][0]:5.4f}\")\n",
    "    \n",
    "progress.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем несколько предсказаний для примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  11.8 , predicted: 0.0\n",
      "Target:  11.0 , predicted: 0.0\n",
      "Target:  23.7 , predicted: 0.0\n",
      "Target:  35.4 , predicted: 0.0\n",
      "Target:  15.2 , predicted: 0.0\n",
      "Target:  24.4 , predicted: 0.0\n",
      "Target:  33.4 , predicted: 0.0\n",
      "Target:  31.6 , predicted: 0.0\n",
      "Target:  13.4 , predicted: 0.0\n",
      "Target:  34.9 , predicted: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    y_hat = predict_fn(x_train[i], thetas)\n",
    "    print(\"Target: \", y_train[i][0], \", predicted:\", y_hat[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скорее всего, в результате получится довольно большое значение ошибки и предсказания будут не очень точными. Как поднять точность, мы рассмотрим далее :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
